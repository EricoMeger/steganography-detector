%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

\usepackage{include/template/sbc-template}

\usepackage{graphicx,url}

\usepackage{booktabs}

\usepackage{float}

% \usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}  

\renewcommand\refname{Referências} % Troca nome da secao de references para referencias
\renewcommand\tablename{Tabela}

\sloppy

\title{Estudo sobre Ferramentas de Software Livre para Detecção de Esteganografia em Imagens Digitais}

\author{Érico Meger\inst{1}, Eros Henrique Lunardon Andrade\inst{1}, Guilherme Werneck de Oliveira\inst{1}}

\address{Campus Pinhais – Instituto Federal do Paraná (IFPR)\\
Pinhais - PR - Brasil\\
\email{ericomeger9@gmail.com, eroshla@hotmail.com, guilherme.werneck@ifpr.edu.br}}

\begin{document}

\maketitle

\begin{abstract}
  This work analyzes free and open-source tools for image steganography and steganalysis,
  covering both traditional methods and modern approaches based on convolutional neural networks (CNNs).
  Classical tools such as Aletheia and StegExpose provide accessibility and transparency, yet show limitations when facing advanced steganographic algorithms.
  Conversely, CNN-based models including SRNet, EfficientNet, and MixNet achieve superior performance by automatically learning subtle modification patterns
  but require significant computational resources. The analysis reveals a paradigm shift in the field and highlights the importance of open-source software
  in enabling research.
  The study concludes that future steganalysis solutions must balance accuracy, efficiency, and usability.
\end{abstract}

\begin{resumo}
  Este trabalho analisa ferramentas de software livre voltadas à esteganografia e estegonálise em imagens digitais,
  abrangendo métodos tradicionais e abordagens modernas baseadas em redes neurais convolucionais (CNNs).
  Ferramentas clássicas, como Aletheia e StegExpose,
  oferecem acessibilidade e transparência, porém apresentam limitações frente a algoritmos esteganográficos avançados.
  Já as CNNs como SRNet, EfficientNet e
  MixNet demonstram desempenho superior ao aprenderem automaticamente padrões sutis de modificação, embora dependam de alto poder computacional.
  A análise evidencia uma transição de paradigma na área e destaca o papel do software livre na democratização da pesquisa. Conclui-se que o futuro da estegonálise
  requer soluções que combinem precisão, eficiência e usabilidade.

\end{resumo}

\section{Introdução}

O movimento do software livre se estabelece como um paradigma essencial para
promover transparência, colaboração e inovação no cenário tecnológico
contemporâneo. Segundo a Free Software Foundation, software livre é definido
pela sua capacidade de respeitar as liberdades e o controle dos usuários sobre
o software: a liberdade de executar o programa para qualquer propósito, de
estudá-lo e modificá-lo (acesso ao código-fonte é pré-requisito), de
redistribuir cópias e de distribuir versões modificadas para a comunidade,
conhecidas como as quatro liberdades essenciais \cite{gnu_freesw}.

Ao assegurar essas liberdades, o software livre fortalece a confiança nas
soluções digitais, por permitir auditoria e aprendizado mútuo, e fomenta
ambientes colaborativos dinâmicos, onde ferramentas podem ser aprimoradas
coletivamente.

No contexto da esteganografia, a disponibilidade de ferramentas abertas amplia
as possibilidades de pesquisa e aplicação prática. A análise de imagens
digitais se beneficia de recursos que vão de técnicas clássicas de inspeção
estatística e visual (por exemplo, Aletheia) a detectores automatizados em lote
(como StegExpose), bem como de abordagens recentes de aprendizado de máquina
que elevam a acurácia da detecção.

A esteganografia pode ser compreendida como a técnica de ocultar informações em
meios aparentemente comuns, de forma que um observador externo não consiga
identificar a presença de dados ocultos \cite{Fridrich2010}. Assim, o campo não
se limita ao ato de esconder informações, mas abrange técnicas, algoritmos e
aplicações destinadas a garantir a confidencialidade e a discrição da
comunicação. Em contraste com a criptografia, que protege o conteúdo sem
ocultar sua existência, a esteganografia busca mascarar o próprio ato de
comunicar \cite{Fridrich2010}. Essa característica a torna estratégica tanto
para usos legítimos (autenticação de documentos, proteção da privacidade)
quanto para usos maliciosos, exigindo análise crítica dentro de um contexto
técnico, social e político mais amplo.

Nesse sentido, ao longo da história, e de forma ainda mais acentuada no cenário
contemporâneo, observa-se o fortalecimento de mecanismos de vigilância e
controle sobre a comunicação digital. Na Europa, por exemplo, esse movimento se
materializa tanto em iniciativas de remoção massiva de conteúdos, com mais de
41 milhões de postagens bloqueadas apenas no primeiro semestre de 2025
\cite{poder3602025}, quanto em pressões políticas para enfraquecer a segurança
criptográfica, como a exigência de um backdoor no iCloud, que levou a Apple a
retirar a opção de criptografia de ponta a ponta de seus serviços no Reino
Unido \cite{guardian2025}. Embora tais medidas sejam frequentemente
justificadas em nome da segurança pública, a ausência de transparência sobre os
critérios de censura e o impacto direto na privacidade digital levantam sérias
preocupações. Nesse contexto, a esteganografia age como uma alternativa
tecnológica de resistência, capaz de proporcionar meios de comunicação
discretos e seguros, reforçando sua relevância sociopolítica e justificando o
aprofundamento de seu estudo.

\subsection{Objetivo}

Compreender como ferramentas de código aberto são usadas no contexto de
esteganografia e estegoanálise em imagens digitais, considerando tanto soluções
tradicionais quanto abordagens recentes baseadas em CNNs. Busca-se compreender
como essas ferramentas se estruturam, quais funcionalidades oferecem e de que
forma contribuem para o avanço da área, identificando tendências, limitações e
possibilidades de integração entre métodos clássicos e modernos.

\section{Revisão bibliográfica} \label{sec:firstpage}
Esta seção apresenta uma revisão das principais ferramentas de software livre
desenvolvidas para a detecção de esteganografia em imagens digitais,
descrevendo suas abordagens, funcionalidades e limitações. Além dessas
ferramentas tradicionais, também são consideradas soluções baseadas em redes
neurais convolucionais (CNNs) de código aberto, como a EfficientNet, que
atualmente representam o estado da arte na estegoanálise
\cite{LaCroix2024survey}.

%trabalho 1

O trabalho \textit{"An Ensemble Model using CNNs on Different Domains for
  ALASKA2 Image Steganalysis"} de Chubachi \cite{chubachi2020cnn} surge da
constatação de que muitos detectores de esteganografia baseados em aprendizado
profundo não generalizam bem em cenários reais devido ao uso de conjuntos de
dados simplificados. A competição ALASKA2 ofereceu um ambiente mais realista,
com imagens JPEG coloridas de diferentes origens e processos, estimulando
soluções mais aplicáveis. Nesse contexto, o objetivo do autor foi desenvolver
um modelo de detecção baseado em um ensemble de redes convolucionais que
combinasse informações tanto do domínio espacial (RGB, YUV e Lab) quanto do
domínio da frequência (coeficientes DCT).

A metodologia proposta envolveu CNNs construídas sobre arquiteturas
EfficientNet, com ajustes para lidar com as especificidades de cada domínio. No
caso dos coeficientes DCT, foram aplicadas codificações one-hot, recortes de
valores e convoluções dilatadas para capturar padrões. Para integrar os
modelos, além da simples média de previsões, foi desenvolvido um perceptron
multicamada capaz de combinar os mapas de características. Também se utilizaram
técnicas auxiliares, como pseudo-rotulagem e stacking com LightGBM. Em
experimentos conduzidos com 300 mil imagens, o uso combinado dos modelos trouxe
ganhos consistentes, resultando em uma performance de AUC ponderado próxima de
0,94 e garantindo a terceira colocação na competição.

O estudo apresenta como pontos fortes a inovação de combinar diferentes
domínios e a validação em um cenário competitivo e realista. Contudo, o alto
custo computacional e a limitação de testar apenas algoritmos de esteganografia
já conhecidos restringem sua aplicabilidade prática.

%trabalho 2

Explorando ensembles de forma mais sistemática, o estudo \textit{Ensemble of
  CNNs for Steganalysis: An Empirical Study} de Xu et al. \cite{xu2016ensemble}
retorna ao tema de ensembles, mas com uma abordagem mais empírica. Motivado
pela observação de que, embora as Redes Neurais Convolucionais (CNNs)
estivessem ganhando popularidade em esteganálise, a maioria das pesquisas se
concentrava no design de um único modelo de CNN. No entanto, no campo mais
amplo da visão computacional e do aprendizado de máquina, as melhores
performances são consistentemente alcançadas por meio de ensembles, ou seja, a
combinação de múltiplos modelos. Os autores perceberam uma lacuna na literatura
de esteganálise, que ainda não havia explorado a fundo estratégias de ensemble
mais sofisticadas do que a simples média das previsões. O principal objetivo do
trabalho foi, portanto, conduzir um estudo empírico para avaliar o desempenho
de diferentes estratégias de combinação de CNNs para a tarefa de esteganálise,
buscando ir além da média de modelos e testar o uso de classificadores de
segundo nível treinados sobre as saídas e representações internas das redes.

A metodologia proposta envolveu, primeiramente, o treinamento de um conjunto de
16 CNNs individuais, que serviram como "aprendizes de base", cada uma treinada
sobre um subconjunto aleatório do dataset de treinamento. A partir disso, os
autores testaram e compararam três abordagens de ensemble: a média simples das
probabilidades de saída, a criação de novos vetores de características a partir
dessas probabilidades para treinar um classificador de segundo nível, e uma
terceira técnica mais inovadora que consistia em extrair as representações de
características das camadas intermediárias de cada CNN, concatená-las e usá-las
para treinar o classificador de segundo nível. O experimento foi realizado no
dataset BOSSbase v1.01 para detectar a esteganografia do algoritmo S-UNIWARD
com uma taxa de inserção de 0.4 bpp.

O trabalho apresenta como pontos fortes a demonstração sistemática de que o uso
de um classificador de segundo nível sobre as saídas das CNNs melhora
consistentemente o desempenho em relação à média de modelos, e, principalmente,
a descoberta de que o uso das representações de características intermediárias
resulta na melhor performance, indicando que os classificadores mais robustos
podem extrair padrões mais discriminativos do que as camadas de classificação
simples das CNNs base. As limitações do estudo, no entanto, incluem a sua
validação em apenas um dataset, contra um único algoritmo esteganográfico e com
uma única taxa de inserção, além do alto custo computacional da abordagem.

%trabalho 3

Complementando a abordagem de ensemble, o trabalho \textit{ImageNet Pre-trained
  CNNs for JPEG Steganalysis} de Yousfi et al. \cite{fridrich2020imagenet}
explora uma direção diferente: o uso de transfer learning em esteganálise. A
principal motivação para o estudo surgiu a partir da competição de esteganálise
ALASKA II, onde foi observado que os participantes com melhor desempenho
utilizavam modelos de visão computacional de uso geral, como EfficientNet e
ResNet, em vez de arquiteturas especializadas e treinadas do zero para a
tarefa. Essa nova abordagem, baseada em aprendizagem por transferência
(transfer learning), representava uma mudança de paradigma em relação a modelos
consolidados, como a SRNet, que eram projetados especificamente para
esteganálise. Diante desse cenário, o principal objetivo dos autores foi
investigar e demonstrar formalmente a eficácia e a superioridade desses modelos
pré-treinados no ImageNet para a detecção de esteganografia em imagens JPEG,
comparando seu desempenho com as abordagens tradicionais.

A metodologia utilizada centrou-se em refinar (fine-tuning) diversas
arquiteturas pré-treinadas, como EfficientNet, MixNet e ResNet, no conjunto de
dados da ALASKA II. Os autores também conduziram experimentos para avaliar o
impacto de decisões arquitetônicas, como a remoção de camadas de pooling ou
stride no início da rede, confirmando que a manutenção da resolução original
nas primeiras camadas é crucial para a performance em esteganálise. O
experimento principal foi conduzido no dataset da ALASKA II, que continha
imagens comprimidas com fatores de qualidade 75, 90 e 95 e com mensagens
ocultas pelos algoritmos J-UNIWARD, J-MiPOD e UERD.

Os pontos fortes dessa abordagem, destacados no artigo, são a acurácia
superior, a maior eficiência de dados e uma velocidade de treinamento ordens de
magnitude mais rápida em comparação com o treinamento de um modelo do zero.
Como limitação, os próprios autores apontam que o estudo foi amplamente focado
no ambiente da ALASKA II e que o ganho de desempenho obtido com o
pré-treinamento tende a diminuir à medida que o volume de dados para
treinamento na tarefa final aumenta.

%trabalho 4
Diferentemente das abordagens anteriores que se concentram em arquiteturas de
ensemble ou transfer learning, o artigo \textit{An Intriguing Struggle of CNNs
  in JPEG Steganalysis and the OneHot Solution} de Yousfi e Fridrich
\cite{fridrichOnehot} identifica e resolve uma limitação específica das CNNs em
esteganálise. A pesquisa parte da descoberta de cenários específicos onde as
modernas CNNs, como a SRNet, apresentavam um desempenho surpreendentemente
inferior ao de métodos mais antigos baseados em extração de características,
como o JPEG Rich Model (JRM). Essa falha era particularmente evidente na
detecção do algoritmo nsF5 e do J-UNIWARD em certos tipos de imagem JPEG, e a
análise revelou que o sucesso do JRM nesses casos se devia à sua capacidade de
computar estatísticas simples dos coeficientes DCT, como histogramas de
coocorrência, algo que as CNNs convencionais, que operam na imagem
descomprimida, não conseguiam "enxergar".

A metodologia se baseia em duas inovações principais. A primeira é a própria
rede "OneHot CNN", que transforma sua entrada através de uma camada de
"codificação one-hot com corte" (clipped one-hot encoding). Nessa etapa, os
valores absolutos dos coeficientes DCT são transformados em um volume binário
que facilita o aprendizado de ocorrências e coocorrências pelas camadas
convolucionais subsequentes. A segunda inovação é a arquitetura de ramo duplo
"OneHot+SRNet", que mescla a nova rede OneHot com uma CNN convencional (SRNet).
O experimento principal consistiu em testar o desempenho dessas novas
arquiteturas nos cenários problemáticos (nsF5 e J-UNIWARD) em datasets como
BOSSbase e BOWS2.

O trabalho se destaca por identificar com precisão uma falha em modelos estado
da arte e propor uma solução elegante e eficaz, a codificação one-hot, que
permite a uma CNN aprender estatísticas de alta ordem de forma flexível. A
arquitetura de ramo duplo é outro ponto forte, pois oferece uma maneira prática
de criar um detector mais completo e robusto. Uma limitação implícita é que a
rede OneHot é altamente especializada para esses casos de falha, o que
justifica sua fusão com uma rede mais geral como a SRNet.

%trabalho 5
Para contextualizar os trabalhos anteriores dentro do panorama geral da área, o
artigo \textit{Comprehensive survey on image steganalysis using deep learning}
de De La Croix et al. \cite{LaCroix2024survey} oferece uma visão abrangente do
estado da arte. A principal motivação dos autores reside na observação de que
as abordagens tradicionais, baseadas em aprendizado de máquina (ML), se
mostraram ineficazes contra os modernos algoritmos de esteganografia. Métodos
clássicos como Support Vector Machines (SVM) e Ensemble Classifiers (EC)
dependem de um processo árduo e manual de extração de características, o que
não só consome tempo, mas também sofre com a "maldição da dimensionalidade",
onde o excesso de características prejudica o desempenho do classificador. A
introdução do aprendizado profundo (deep learning) marcou uma mudança de
paradigma, unificando a extração de características e a classificação em um
único processo otimizado e de ponta a ponta.

A metodologia empregada pelos autores é a de uma revisão sistemática da
literatura, baseada no protocolo PRISMA. Eles selecionaram 24 artigos de ponta,
publicados entre 2014 e 2023, que representam a vanguarda da esteganálise com
deep learning. O artigo estrutura-se de forma didática, iniciando com uma
taxonomia detalhada das técnicas de esteganálise, explorando a transição do
paradigma de ML para o de deep learning, e analisando as arquiteturas de Redes
Neurais Convolucionais (CNNs) propostas, desde as pioneiras até as mais
recentes. A pesquisa documenta a progressão das arquiteturas, começando com
modelos como Qian-Net, passando pelo Xu-Net, chegando a modelos mais
sofisticados como o Ye-Net, e culminando em arquiteturas estado da arte como a
SRNet e a GBRAS-Net.

Um dos pontos fortes mais significativos desta revisão é a identificação dos
principais desafios que ainda persistem no campo da esteganálise com deep
learning: vulnerabilidade a ataques adversariais, qualidade e padronização dos
datasets, ineficiência para lidar com imagens de tamanhos arbitrários,
dificuldade na detecção de baixo payload, problema do cover-source mismatch,
dificuldade na identificação de características globais, e necessidade de um
grande número de amostras de treinamento. Como limitação, a revisão foca, que
principalmente em imagens no domínio espacial, deixando lacunas para outros
domínios como JPEG.

%trabalho 6
A pesquisa de Farooq e Selwal \cite{farooq2023systematic} é motivada pela
crescente complexidade e sofisticação tanto das técnicas de esteganografia
quanto das de esteganálise. Os autores partem da premissa de que, com os
avanços em comunicação e tecnologia da informação, os métodos para ocultar
dados em imagens tornaram-se mais robustos, desafiando as abordagens de
detecção tradicionais \cite{farooq2023systematic}. Enquanto métodos clássicos
se baseiam em um processo de duas etapas, extração manual de características
(como os Rich Models) e classificação (usando SVMs ou Ensemble Classifiers), o
aprendizado profundo (deep learning, DL) emergiu como uma alternativa superior,
unificando essas etapas e aprendendo automaticamente as características mais
relevantes diretamente dos dados brutos \cite{farooq2023systematic}. Essa
mudança de paradigma gerou uma vasta quantidade de novas arquiteturas e
abordagens, criando a necessidade de uma análise consolidada. Assim, o
principal objetivo do trabalho é realizar uma revisão sistemática e aprofundada
das técnicas de esteganálise de imagens baseadas em DL, oferecendo um panorama
do estado da arte, analisando comparativamente os modelos, os datasets de
referência e as métricas de avaliação, além de identificar os desafios de
pesquisa que permanecem abertos para guiar futuros investigadores no campo
\cite{farooq2023systematic}.

A metodologia adotada no artigo é a de uma revisão sistemática e abrangente da
literatura. Primeiramente, os autores fornecem uma classificação detalhada das
técnicas de esteganálise, cobrindo desde ataques visuais e estatísticos até
abordagens mais complexas como a análise estrutural e métodos baseados em redes
neurais artificiais (ANN), estabelecendo um contexto histórico e técnico
\cite{farooq2023systematic}. O foco principal, no entanto, é a análise
cronológica das abordagens baseadas em DL, com especial atenção às Redes
Neurais Convolucionais (CNNs). O artigo revisa uma sequência de trabalhos
influentes, analisando a arquitetura da rede, as funções de ativação
utilizadas, as bases de dados (como BOSSBase, BOWS2 e ImageNet), os algoritmos
esteganográficos alvo (S-UNIWARD, WOW, HILL, etc.) e os resultados de
desempenho reportados. Além disso, a pesquisa compila e discute os principais
datasets de referência, as métricas de avaliação quantitativas (como taxa de
erro, curva ROC, AUC e WAUC) e qualitativas (PSNR, SSIM), e até mesmo lista
ferramentas de software de código aberto e proprietárias disponíveis para
esteganálise \cite{farooq2023systematic}.

Como um artigo de revisão, o experimento consiste na compilação e na análise
crítica dos resultados publicados por outros pesquisadores. A pesquisa
consolida os dados de desempenho de diversas arquiteturas de CNN em uma tabela
comparativa detalhada, avaliando a taxa de erro de detecção contra diferentes
algoritmos esteganográficos (como HUGO, WOW, S-UNIWARD) e em diferentes taxas
de inserção de dados (payloads) \cite{farooq2023systematic}. A análise dos
resultados mostra uma clara evolução: os primeiros modelos de CNN apresentavam
desempenho comparável ou ligeiramente inferior aos métodos tradicionais (como o
SRM com Ensemble Classifier), mas com o avanço no design das arquiteturas,
incorporando camadas de pré-processamento com filtros, funções de ativação mais
adequadas (como a TLU), normalização em lote (BN), e arquiteturas residuais
profundas, os modelos mais recentes superaram significativamente os métodos
clássicos, especialmente em cenários com maior payload
\cite{farooq2023systematic}. O artigo destaca, por exemplo, o desempenho
superior de modelos como o de Yedroudj et al. e a SRNet, que estabeleceram
novos padrões de acurácia na detecção \cite{farooq2023systematic}.

Embora as revisões sistemáticas e as arquiteturas de redes neurais definam a
fronteira do conhecimento teórico, a aplicação forense efetiva exige a tradução
desses avanços em ferramentas de software utilizáveis e auditáveis. Nesse
sentido, destacam-se iniciativas de código aberto que buscam implementar tanto
técnicas clássicas quanto abordagens modernas de aprendizado profundo.

O trabalho \textit{"Aletheia: an open-source toolbox for steganalysis"} de
Lerch-Hostalot e Megías \cite{aletheia} aborda a necessidade de ferramentas que
acompanhem a evolução das ameaças esteganográficas, especificamente aquelas que
utilizam aprendizado profundo e que operam em cenários reais onde ocorre o
problema do \textit{Cover Source Mismatch} (CSM). Os autores identificaram que,
embora o interesse em esteganálise baseada em \textit{Deep Learning} tenha
crescido, faltavam ferramentas que permitissem aos analistas aplicar esses
modelos modernos de forma prática e aos pesquisadores comparar métodos de
maneira padronizada. O objetivo do Aletheia é preencher essa lacuna, fornecendo
uma plataforma unificada para simulação, treinamento e detecção.

A metodologia do Aletheia consiste em uma caixa de ferramentas modular
desenvolvida em Python que incorpora simuladores para algoritmos de
esteganografia comuns (como HILL e S-UNIWARD) e modelos de classificação
baseados em redes neurais eficientes. O software implementa ataques estruturais
e estatísticos, mas seu diferencial reside na inclusão de técnicas específicas
para lidar com o CSM, como o uso de \textit{DCI (Detection of Classifier
  Inconsistencies)}, permitindo que o sistema mantenha a acurácia mesmo quando as
propriedades estatísticas da fonte da imagem divergem das do treinamento. Além
disso, a ferramenta oferece funcionalidades de calibração automática e
preparação de datasets para experimentos controlados.

O principal ponto forte do Aletheia é ser, segundo os autores, a única
ferramenta de esteganálise disponível publicamente que incorpora técnicas
modernas para mitigação de CSM, tornando-a particularmente relevante para
cenários forenses do mundo real onde a fonte da imagem é desconhecida.

Focando na eficiência do processamento em massa, a dissertação
\textit{"StegExpose: A Tool for Detecting LSB Steganography"} de Boehm
\cite{stegexpose} propõe uma abordagem voltada para a velocidade e a fusão de
detectores. A motivação do trabalho surgiu da constatação de que, em
investigações forenses reais, o analista frequentemente lida com milhares de
arquivos, tornando inviável o uso de métodos computacionalmente custosos para
uma triagem inicial. O objetivo foi criar uma ferramenta capaz de realizar
esteganálise em lote (bulk analysis) em imagens lossless (PNG e BMP),
combinando múltiplos algoritmos para equilibrar precisão e tempo de execução.

A metodologia do StegExpose baseia-se na técnica de fusão de detectores,
integrando quatro métodos clássicos de análise LSB: \textit{Primary Sets},
\textit{Sample Pairs}, \textit{RS Analysis} e \textit{Chi Square}. O autor
desenvolveu um algoritmo denominado "Fast Fusion", que opera em estágios: se um
arquivo é considerado limpo pelos detectores mais rápidos e menos precisos, ele
é descartado da análise subsequente, economizando recursos computacionais. A
ferramenta foi validada em um conjunto de 15.200 imagens provenientes do
Flickr, comparando a precisão da fusão padrão (média aritmética das
probabilidades) contra a fusão rápida.

O estudo contribui ao demonstrar que a fusão de detectores supera a performance
individual de cada componente, com a "fusão padrão" atingindo uma AUC de
aproximadamente 95,8\%, enquanto a "fusão rápida" oferece uma troca aceitável,
sendo mais de três vezes mais rápida com uma perda marginal de precisão. A
limitação óbvia do StegExpose é seu escopo restrito à esteganografia LSB em
domínio espacial e o uso de métodos pré-\textit{Deep Learning}, o que o torna
ineficaz contra algoritmos modernos adaptativos ou esteganografia em JPEG, mas
ele permanece um exemplo seminal de engenharia de software para triagem
forense.

Enquanto os trabalhos revisados concentram-se predominantemente na proposição
de novas arquiteturas ou na validação de algoritmos específicos, esta pesquisa
trás como objetivo central não apenas revisar a literatura, mas analisar como o
software livre viabiliza a aplicação prática desses métodos, contrastando a
acessibilidade das ferramentas tradicionais com a complexidade e o desempenho
dos modernos modelos baseados em redes neurais.

\section{Metodologia}
A pesquisa será conduzida de forma qualitativa e descritiva. Inicialmente, será
realizada uma revisão bibliográfica sobre os fundamentos da esteganografia e da
esteganálise, com ênfase em métodos e algoritmos empregados na detecção de
dados ocultos em imagens digitais. Em seguida, serão mapeadas e analisadas
ferramentas de software livre representativas dessas duas abordagens:
ferramentas que oferecem recursos práticos de inserção e detecção de mensagens,
e soluções recentes baseadas em aprendizado profundo, que utilizam redes
neurais convolucionais para aprimorar a acurácia da detecção. A análise
considerará aspectos como objetivos das ferramentas, funcionalidades
disponibilizadas, técnicas implementadas, documentação, acessibilidade e
relevância no contexto atual da pesquisa em esteganálise. Por fim, será
realizada uma discussão comparativa entre os diferentes tipos de ferramentas,
destacando suas complementaridades e os caminhos evolutivos da área.

\section{Resultados e Discussões}

A análise foi conduzida a partir de um levantamento de ferramentas de software
livre voltadas à esteganografia e à esteganálise em imagens digitais. As
ferramentas foram agrupadas em duas categorias principais: (i) soluções
tradicionais, baseadas em métodos clássicos de inserção e detecção de mensagens
ocultas, e (ii) soluções modernas, baseadas em redes neurais convolucionais
(CNNs), que representam o atual estado da arte na detecção automática de
esteganografia. Os critérios de análise consideraram aspectos como objetivo,
abordagem técnica, tipo de licença, disponibilidade de código, documentação,
acessibilidade e relevância para pesquisa e desenvolvimento.

\subsection{Ferramentas tradicionais}

As ferramentas tradicionais representam a base histórica da esteganografia
prática, fornecendo utilitários acessíveis para detecção de dados ocultos.

\begin{table}[H]
  \centering
  \caption{Ferramentas tradicionais de software livre para estegoanálise.}
  \label{tab:tradicionais}
  \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{p{4cm} p{3cm} p{4cm} p{2cm}}
      \toprule
      \textbf{Ferramenta}          & \textbf{Objetivo}                               & \textbf{Abordagem Técnica}                              & \textbf{Licença} \\
      \midrule
      Aletheia \cite{aletheia}     & Estegoanálise estatística e visual              & Análise de ruído, histogramas e artefatos de compressão & MIT              \\
      \addlinespace
      StegExpose \cite{stegexpose} & Detecção automatizada de esteganografia em lote & Combina métricas como Sample Pair e RS Analysis         & Não especificada \\
      \bottomrule
    \end{tabular}
  }
\end{table}

A Tabela \ref{tab:tradicionais} apresenta um resumo comparativo das principais
soluções software livre de estegoanálise em imagem analisadas.

A Aletheia revela seu duplo propósito como uma ferramenta para analistas
forenses e pesquisadores. Suas funcionalidades vão além da simples detecção,
incorporando ataques estruturais a formatos de imagem, análise de metadados e,
principalmente, a capacidade de realizar análises visuais e estatísticas. Por
exemplo, a ferramenta permite a extração de histogramas de cores e a aplicação
de filtros high-pass para amplificar ruídos, auxiliando um analista a
identificar visualmente anomalias que podem indicar a presença de dados
ocultos. Um diferencial importante da Aletheia é sua capacidade de lidar com o
desafio do Cover Source Mismatch (CSM), um cenário comum em investigações do
mundo real onde o modelo de detecção não foi treinado com o mesmo tipo de
imagem que está sendo analisada. No entanto, sua principal limitação para
usuários não especializados é a dependência da interpretação manual dos
resultados, exigindo conhecimento técnico para tirar conclusões eficazes
\cite{aletheia}.

O StegExpose, por sua vez, foi projetado com outro objetivo: a detecção
automatizada e em lote de esteganografia LSB em imagens de formato lossless
(sem perdas), como PNG e BMP. Sua principal inovação técnica é o uso de fusion
techniques (técnicas de fusão), que combinam os resultados de múltiplos
detectores estatísticos conhecidos, como RS Analysis\footnote{A análise RS
  (Regular/Singular groups) explora como a inserção de dados LSB afeta a contagem
  de grupos de pixels regulares e singulares, revelando anomalias estatísticas.}
e Sample Pair Analysis\footnote{A Sample Pair Analysis examina pares de pixels
  e como seus valores mudam com a inserção de dados, detectando desvios em
  relação a uma imagem limpa.}, para produzir uma classificação final mais
precisa do que cada método individualmente. A ferramenta foi desenvolvida com
foco em praticidade e velocidade, oferecendo dois modos de operação: um modo
"padrão", que maximiza a precisão, e um modo "rápido", que otimiza o tempo de
análise ao descartar arquivos considerados "limpos" nos estágios iniciais do
processo. Sua principal limitação, reconhecida no próprio trabalho de origem, é
ser especializada apenas em esteganografia LSB, não sendo projetada para
detectar métodos mais avançados que operam no domínio da frequência (como em
arquivos JPEG) \cite{stegexpose}.

Em comum, ambas ferramentas têm como principal mérito a acessibilidade e a
transparência, características garantidas pela Aletheia, por conta de sua
licença de software livre e pelo StegExpose que, apesar de não definir uma
licença específica, é um projeto de código aberto. Além disso, ambas
ferramentas contribuem para o ensino e a experimentação prática, permitindo a
inspeção de algoritmos e a reprodutibilidade de resultados. No entanto, sua
precisão na detecção é limitada, principalmente quando confrontadas com métodos
modernos baseados em aprendizado profundo.

\subsection{Abordagens baseadas em redes neurais convolucionais (CNNs)}

Com o avanço da inteligência artificial, modelos de aprendizado profundo
passaram a ser adotados para estegoanálise, com desempenho superior às
abordagens anteriores \cite{Fridrich2019SRNet}.

\begin{table}[H]
  \centering
  \caption{Redes neurais de código aberto aplicadas à estegoanálise.}
  \label{tab:cnns}
  \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{p{4cm} p{3cm} p{4cm} p{2cm}}
      \toprule
      \textbf{Modelo}                                                   & \textbf{Arquitetura Base}                       & \textbf{Aplicação em Estegoanálise}                          & \textbf{Licença} \\
      \midrule
      SRNet \cite{Fridrich2019SRNet}                                    & CNN profunda especializada em ruído residual    & Detecção de padrões sutis de modificação em pixels           & MIT              \\
      \addlinespace
      EfficientNet \cite{tan2019efficientnet, fridrich2021EfficientNet} & CNN escalável baseada em compound scaling       & Fine-tuning em datasets como ALASKA II, com alto desempenho  & Apache-2.0       \\
      \addlinespace
      MixNet \cite{tan2019mixnet}                                       & Variante da EfficientNet com convoluções mistas & Melhor generalização e leveza em comparação com EfficientNet & Apache-2.0       \\
      \bottomrule
    \end{tabular}
  }
\end{table}

A Tabela \ref{tab:cnns} apresenta uma síntese das principais redes utilizadas
na área, destacando suas licenças e relevância como ferramentas de pesquisa.
Esses modelos representam uma evolução metodológica na área: ao contrário das
ferramentas tradicionais, que dependem de heurísticas e análise manual, as CNNs
aprendem a identificar padrões de esteganografia diretamente dos dados.

A SRNet (Steganalysis Residual Network), por exemplo, foi um marco por ser uma
das primeiras arquiteturas profundas projetadas especificamente para
estegoanálise. Sua principal inovação foi a criação de um bloco de camadas
convolucionais sem pooling (agrupamento) e com conexões residuais, projetado
para extrair e amplificar o "ruído residual", traços deixados pela inserção de
dados, sem suprimir esse sinal fraco, algo que arquiteturas de classificação de
imagem convencionais não fazem eficientemente. Ao ser "agnóstica" a
heurísticas, a SRNet demonstrou a superioridade do aprendizado de
características de ponta a ponta (end-to-end) e se tornou uma baseline
essencial para a comparação de novos modelos na área \cite{Fridrich2019SRNet}.

Posteriormente, a EfficientNet introduziu uma nova abordagem, não para a
estegoanálise, mas para a eficiência de CNNs em geral. Sua arquitetura,
encontrada por meio de busca de arquitetura neural (NAS), propõe um método de
escalonamento composto (compound scaling) que equilibra de forma otimizada a
profundidade, a largura e a resolução da rede. Pesquisadores em estegoanálise
rapidamente adaptaram essa ideia, aplicando fine-tuning em modelos EfficientNet
pré-treinados em grandes datasets como o ImageNet. Essa técnica provou ser
altamente eficaz, alcançando ótimo desempenho em benchmarks de estegoanálise
como o ALASKA II, demonstrando que uma arquitetura eficiente para visão
computacional geral também pode ser uma poderosa ferramenta para detectar
padrões esteganográficos \cite{tan2019efficientnet}.

Derivada da EfficientNet, a MixNet aprimora ainda mais a eficiência ao
introduzir a Mixed Depthwise Convolution (MixConv). Em vez de usar um único
tamanho de kernel convolucional por camada (ex: 3x3 ou 5x5), a MixConv combina
múltiplos tamanhos de kernel em uma única operação. Essa abordagem permite que
o modelo capture padrões em diferentes escalas e resoluções simultaneamente,
resultando em melhor precisão e generalização com um custo computacional ainda
menor. Para a estegoanálise, isso representa uma direção promissora para o
desenvolvimento de modelos que sejam não apenas precisos, mas também leves o
suficiente para serem integrados em aplicações práticas e com recursos
limitados \cite{tan2019mixnet}.

No entanto, durante o levantamento bibliográfico realizado para esta pesquisa,
notou-se que a MixNet foi a arquitetura menos frequentemente explorada.

\subsection{Síntese comparativa e tendências}

A análise das ferramentas mostra uma transição de paradigma na estegoanálise:
uma passagem de métodos estatísticos, dependentes de interpretação humana, para
abordagens automatizadas baseadas em aprendizado profundo. De um lado,
ferramentas tradicionais como a Aletheia e o StegExpose democratizam o acesso à
estegoanálise com foco em usabilidade e análise forense direcionada. A
Aletheia, com suas análises visuais, serve como uma excelente ferramenta
didática e investigativa, enquanto o StegExpose, com sua detecção em lote,
oferece uma solução prática para varreduras em larga escala contra ataques LSB.
Ambas, no entanto, atingem um teto de precisão por dependerem de heurísticas e
características pré-definidas.

Do outro lado, modelos de CNN como a SRNet e adaptações da EfficientNet
representam o estado da arte em detecção. A SRNet inaugurou a era do
aprendizado de características de ponta a ponta, provando que uma rede pode
aprender a "ver" o ruído esteganográfico de forma mais eficaz que qualquer
engenharia de características manual. A subsequente adoção da EfficientNet
demonstram uma segunda tendência: a busca por eficiência. Contudo, essa
evolução metodológica impõe uma nova barreira de entrada. O treinamento de tais
modelos exige não apenas vastos conjuntos de dados, mas também um poder
computacional significativo, envolvendo hardware especializado (como GPUs ou
TPUs) e longos períodos de processamento. Esse requisito de recursos torna a
replicação e o desenvolvimento de novas arquiteturas um desafio considerável
para pesquisadores e desenvolvedores com acesso limitado a infraestrutura de
alto desempenho, restringindo, em certo grau, a mesma democratização que as
ferramentas clássicas promoveram.

O princípio do software livre é um fator comum e essencial para o avanço de
ambas as abordagens. Nas abordagens tradicionais, ele garante transparência e
acessibilidade. Nas abordagens modernas, seu papel é ainda mais crucial. É, que
o caráter aberto de frameworks como TensorFlow e PyTorch que viabiliza a
implementação, o compartilhamento e a rápida iteração sobre arquiteturas como
as analisadas. Essa cultura de colaboração permite que a comunidade científica
construa sobre o trabalho alheio, adaptando uma EfficientNet para estegoanálise
ou propondo uma MixNet, acelerando o ciclo de inovação de uma forma que seria
difícil de ver em um ecossistema de código fechado.

Por fim, a análise aponta para desafios persistentes e tendências futuras. A
principal lacuna identificada é a ausência de ferramentas que integrem a alta
precisão das CNNs com a usabilidade das soluções clássicas. Enquanto modelos
como a MixNet apontam para um futuro de detectores leves e eficientes, eles
permanecem, em grande parte, no domínio acadêmico. O próximo passo evolutivo
para a estegoanálise de software livre parece ser, portanto, o desenvolvimento
de soluções que unam esses dois mundos, criando ferramentas que sejam, ao mesmo
tempo, poderosas, acessíveis e práticas para um público mais amplo.

\section{Conclusão}\label{sec:figs}

O presente estudo atingiu seu objetivo de analisar ferramentas de código aberto
voltadas para a esteganografia e esteganálise, compreendendo tanto as soluções
tradicionais quanto as baseadas no estado da arte em Redes Neurais
Convolucionais.

A análise comparativa evidenciou uma transição de paradigma na área. Por um
lado, as ferramentas tradicionais, como Aletheia e StegExpose, cumprem um papel
fundamental na democratização do conhecimento, oferecendo interfaces acessíveis
e transparência algorítmica essenciais para o ensino e para a análise forense
preliminar. Contudo, sua dependência de heurísticas manuais impõe um teto de
desempenho quando comparado a abordagens baseadas em redes neurais.

Por outro lado, as abordagens baseadas em CNNs, exemplificadas pela SRNet e
pelas adaptações da EfficientNet, demonstraram superioridade técnica, com
aprendizado autônomo. Entretanto, observou-se que essa evolução técnica trouxe
consigo novos desafios: alto custo computacional e demanda por grandes volumes
de dados para treinamento. A exigência de hardware de alto desempenho para o
treinamento desses modelos restringe a acessibilidade que o software livre visa
promover, limitando a reprodutibilidade e criando um distanciamento entre a
precisão acadêmica e a aplicação prática por usuários comuns.

Conclui-se, portanto, que o software livre atua como um alicerce para ambas as
vertentes, fomentando a colaboração e acelerando a inovação através de
frameworks abertos. O futuro da esteganálise não reside apenas na busca por
maior precisão, mas na eficiência e usabilidade. A tendência aponta para o
desenvolvimento de arquiteturas, que cada vez mais aumentem a acessibilidade e
auditabilidade da segurança da informação.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=.5\textwidth]{fig1.jpg}
% \caption{A typical figure}
% \label{fig:exampleFig1}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=.3\textwidth]{fig2.jpg}
% \caption{This figure is an example of a figure caption taking more than one
%   line and justified considering margins mentioned in Section~\ref{sec:figs}.}
% \label{fig:exampleFig2}
% \end{figure}

% \begin{table}[ht]
% \centering
% \caption{Variables to be considered on the evaluation of interaction
%   techniques}
% \label{tab:exTable1}
% \includegraphics[width=.7\textwidth]{table.jpg}
% \end{table}

\bibliographystyle{include/template/sbc}
\bibliography{include/bibliography/bibliography}

\end{document}
