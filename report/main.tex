%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

\usepackage{include/template/sbc-template}

\usepackage{graphicx,url}

%\usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}  

\renewcommand\refname{Referências} % Troca nome da secao de references para referencias

\sloppy

\title{Estudo sobre Ferramentas de Software Livre para Detecção de Esteganografia em Imagens Digitais}

\author{Érico Meger\inst{1}, Eros Henrique Lunardon Andrade\inst{1}, Guilherme Werneck de Oliveira\inst{1}}


\address{Campus Pinhais – Instituto Federal do Paraná (IFPR)
Pinhais - PR - Brasil}

\begin{document}

\maketitle

\begin{abstract}
  This meta-paper describes the style to be used in articles and short papers
  for SBC conferences. For papers in English, you should add just an abstract
  while for the papers in Portuguese, we also ask for an abstract in
  Portuguese (``resumo''). In both cases, abstracts should not have more than
  10 lines and must be in the first page of the paper.
\end{abstract}

\begin{resumo}
  Este meta-artigo descreve o estilo a ser usado na confecção de artigos e
  resumos de artigos para publicação nos anais das conferências organizadas
  pela SBC. É solicitada a escrita de resumo e abstract apenas para os artigos
  escritos em português. Artigos em inglês deverão apresentar apenas abstract.
  Nos dois casos, o autor deve tomar cuidado para que o resumo (e o abstract)
  não ultrapassem 10 linhas cada, sendo que ambos devem estar na primeira
  página do artigo.
\end{resumo}

\section{Introdução}

O movimento do software livre se estabelece como um paradigma essencial para
promover transparência, colaboração e inovação no cenário tecnológico
contemporâneo. Segundo a Free Software Foundation, software livre é definido
pela sua capacidade de respeitar as liberdades e o controle dos usuários sobre
o software: a liberdade de executar o programa para qualquer propósito, de
estudá-lo e modificá-lo (acesso ao código-fonte é pré-requisito), de
redistribuir cópias e de distribuir versões modificadas para a comunidade,
conhecidas como as quatro liberdades essenciais \cite{gnu_freesw}.

Ao assegurar essas liberdades, o software livre não apenas fortalece a
confiança nas soluções digitais, por permitir auditoria e aprendizado mútuo,
mas também fomenta ambientes colaborativos dinâmicos, onde ferramentas podem
ser aprimoradas coletivamente. 



%--REVISE--
%this needs to encompass more, not just libraries
Essa filosofia de abertura e colaboração se
manifesta também no campo da inteligência artificial, por meio de bibliotecas
como PyTorch, TensorFlow e scikit-learn. Essas ferramentas de código aberto
democratizam o acesso a algoritmos de aprendizado de máquina, permitindo
reprodutibilidade científica, auditoria de modelos e desenvolvimento
colaborativo de soluções inovadoras \cite{pytorch_about, tensorflow_about}.

No contexto da esteganografia, a disponibilidade dessas ferramentas open source
oferece grandes oportunidades para o avanço da área.

A análise de imagens
digitais, por exemplo, pode se beneficiar de recursos de detecção de padrões e
classificação automática fornecidos por essas ferramentas, auxiliando tanto no
desenvolvimento de técnicas esteganográficas mais robustas quanto na criação de
métodos de detecção mais eficazes. Assim, a intersecção entre software livre,
inteligência artificial e esteganografia evidencia como a filosofia do código
aberto não só fortalece a confiança técnica, mas também amplia as
possibilidades de pesquisa e aplicação prática neste campo.

A esteganografia pode ser compreendida como uma técnica utilizada para esconder
informações em meios aparentemente comuns, de forma que um observador externo
não consiga identificar a presença de dados ocultos \cite{Fridrich2010}.

Essa área de estudo, portanto, não se limita apenas ao ato de esconder
informações, mas constitui um campo de estudo mais amplo que abrange técnicas,
algoritmos e aplicações destinadas a garantir a confidencialidade e a discrição
da comunicação. Em contraste com a criptografia, que protege o conteúdo das
mensagens mas não oculta sua existência, a esteganografia busca mascarar o
próprio ato de comunicação \cite{Fridrich2010}. Essa característica a torna uma
área estratégica tanto para aplicações legítimas, como autenticação de
documentos e proteção da privacidade, quanto para usos maliciosos. Tal
dualidade evidencia que a esteganografia deve ser compreendida não apenas sob
uma perspectiva técnica, mas também dentro de um contexto social e político
mais amplo.

%Does it make sense to have this conclusion for the introduction when we are 
% studying ways to detect steganography?
Nesse sentido, ao longo da história, e de forma ainda mais acentuada no cenário
contemporâneo, observa-se o fortalecimento de mecanismos de vigilância e
controle sobre a comunicação digital. Na Europa, por exemplo, esse movimento se
materializa tanto em iniciativas de remoção massiva de conteúdos, com mais de
41 milhões de postagens bloqueadas apenas no primeiro semestre de 2025
\cite{poder3602025}, quanto em pressões políticas para enfraquecer a segurança
criptográfica, como a exigência de um backdoor no iCloud, que levou a Apple a
retirar a opção de criptografia de ponta a ponta de seus serviços no Reino
Unido \cite{guardian2025}. Embora tais medidas sejam frequentemente
justificadas em nome da segurança pública, a ausência de transparência sobre os
critérios de censura e o impacto direto na privacidade digital levantam sérias
preocupações. Nesse contexto, a esteganografia age como uma alternativa
tecnológica de resistência, capaz de proporcionar meios de comunicação
discretos e seguros, reforçando sua relevância sociopolítica e justificando o
aprofundamento de seu estudo.

\subsection{Objetivo}

Explorar o uso de bibliotecas de software livre no desenvolvimento de modelos
de inteligência artificial para a detecção de esteganografia em imagens
digitais.

\section{Revisão bibliográfica} \label{sec:firstpage}
Essa seção revisará os principais trabalhos relacionados, destacando
contribuições, métodos e limitações que fundamentam o desenvolvimento desta
pesquisa.

%trabalho 1

O trabalho \textit{"An Ensemble Model using CNNs on Different Domains for
  ALASKA2 Image Steganalysis"} de Chubachi \cite{chubachi2020cnn} surge da
constatação de que muitos detectores de esteganografia baseados em aprendizado
profundo não generalizam bem em cenários reais devido ao uso de conjuntos de
dados simplificados. A competição ALASKA2 ofereceu um ambiente mais realista,
com imagens JPEG coloridas de diferentes origens e processos, estimulando
soluções mais aplicáveis. Nesse contexto, o objetivo do autor foi desenvolver
um modelo de detecção baseado em um ensemble de redes convolucionais que
combinasse informações tanto do domínio espacial (RGB, YUV e Lab) quanto do
domínio da frequência (coeficientes DCT).

A metodologia proposta envolveu CNNs construídas sobre arquiteturas
EfficientNet, com ajustes para lidar com as especificidades de cada domínio. No
caso dos coeficientes DCT, foram aplicadas codificações one-hot, recortes de
valores e convoluções dilatadas para capturar padrões. Para integrar os
modelos, além da simples média de previsões, foi desenvolvido um perceptron
multicamada capaz de combinar os mapas de características. Também se utilizaram
técnicas auxiliares, como pseudo-rotulagem e stacking com LightGBM. Em
experimentos conduzidos com 300 mil imagens, o uso combinado dos modelos trouxe
ganhos consistentes, resultando em uma performance de AUC ponderado próxima de
0,94 e garantindo a terceira colocação na competição.

O estudo apresenta como pontos fortes a inovação de combinar diferentes
domínios e a validação em um cenário competitivo e realista. Contudo, o alto
custo computacional e a limitação de testar apenas algoritmos de esteganografia
já conhecidos restringem sua aplicabilidade prática.

%trabalho 2

Explorando ensembles de forma mais sistemática, o estudo \textit{Ensemble of
  CNNs for Steganalysis: An Empirical Study} de Xu et al. \cite{xu2016ensemble}
retorna ao tema de ensembles, mas com uma abordagem mais empírica. Motivado
pela observação de que, embora as Redes Neurais Convolucionais (CNNs)
estivessem ganhando popularidade em esteganálise, a maioria das pesquisas se
concentrava no design de um único modelo de CNN. No entanto, no campo mais
amplo da visão computacional e do aprendizado de máquina, as melhores
performances são consistentemente alcançadas por meio de ensembles, ou seja, a
combinação de múltiplos modelos. Os autores perceberam uma lacuna na literatura
de esteganálise, que ainda não havia explorado a fundo estratégias de ensemble
mais sofisticadas do que a simples média das previsões. O principal objetivo do
trabalho foi, portanto, conduzir um estudo empírico para avaliar o desempenho
de diferentes estratégias de combinação de CNNs para a tarefa de esteganálise,
buscando ir além da média de modelos e testar o uso de classificadores de
segundo nível treinados sobre as saídas e representações internas das redes.

A metodologia proposta envolveu, primeiramente, o treinamento de um conjunto de
16 CNNs individuais, que serviram como "aprendizes de base", cada uma treinada
sobre um subconjunto aleatório do dataset de treinamento. A partir disso, os
autores testaram e compararam três abordagens de ensemble: a média simples das
probabilidades de saída, a criação de novos vetores de características a partir
dessas probabilidades para treinar um classificador de segundo nível, e uma
terceira técnica mais inovadora que consistia em extrair as representações de
características das camadas intermediárias de cada CNN, concatená-las e usá-las
para treinar o classificador de segundo nível. O experimento foi realizado no
dataset BOSSbase v1.01 para detectar a esteganografia do algoritmo S-UNIWARD
com uma taxa de inserção de 0.4 bpp.

O trabalho apresenta como pontos fortes a demonstração sistemática de que o uso
de um classificador de segundo nível sobre as saídas das CNNs melhora
consistentemente o desempenho em relação à média de modelos, e, principalmente,
a descoberta de que o uso das representações de características intermediárias
resulta na melhor performance, indicando que os classificadores mais robustos
podem extrair padrões mais discriminativos do que as camadas de classificação
simples das CNNs base. As limitações do estudo, no entanto, incluem a sua
validação em apenas um dataset, contra um único algoritmo esteganográfico e com
uma única taxa de inserção, além do alto custo computacional da abordagem.

%trabalho 3

Complementando a abordagem de ensemble, o trabalho \textit{ImageNet Pre-trained
  CNNs for JPEG Steganalysis} de Yousfi et al. \cite{fridrich2020imagenet}
explora uma direção diferente: o uso de transfer learning em esteganálise. A
principal motivação para o estudo surgiu a partir da competição de esteganálise
ALASKA II, onde foi observado que os participantes com melhor desempenho
utilizavam modelos de visão computacional de uso geral, como EfficientNet e
ResNet, em vez de arquiteturas especializadas e treinadas do zero para a
tarefa. Essa nova abordagem, baseada em aprendizagem por transferência
(transfer learning), representava uma mudança de paradigma em relação a modelos
consolidados, como a SRNet, que eram projetados especificamente para
esteganálise. Diante desse cenário, o principal objetivo dos autores foi
investigar e demonstrar formalmente a eficácia e a superioridade desses modelos
pré-treinados no ImageNet para a detecção de esteganografia em imagens JPEG,
comparando seu desempenho com as abordagens tradicionais.

A metodologia utilizada centrou-se em refinar (fine-tuning) diversas
arquiteturas pré-treinadas, como EfficientNet, MixNet e ResNet, no conjunto de
dados da ALASKA II. Os autores também conduziram experimentos para avaliar o
impacto de decisões arquitetônicas, como a remoção de camadas de pooling ou
stride no início da rede, confirmando que a manutenção da resolução original
nas primeiras camadas é crucial para a performance em esteganálise. O
experimento principal foi conduzido no dataset da ALASKA II, que continha
imagens comprimidas com fatores de qualidade 75, 90 e 95 e com mensagens
ocultas pelos algoritmos J-UNIWARD, J-MiPOD e UERD.

Os pontos fortes dessa abordagem, destacados no artigo, são a acurácia
superior, a maior eficiência de dados e uma velocidade de treinamento ordens de
magnitude mais rápida em comparação com o treinamento de um modelo do zero.
Como limitação, os próprios autores apontam que o estudo foi amplamente focado
no ambiente da ALASKA II e que o ganho de desempenho obtido com o
pré-treinamento tende a diminuir à medida que o volume de dados para
treinamento na tarefa final aumenta.

%trabalho 4
Diferentemente das abordagens anteriores que se concentram em arquiteturas de
ensemble ou transfer learning, o artigo \textit{An Intriguing Struggle of CNNs
  in JPEG Steganalysis and the OneHot Solution} de Yousfi e Fridrich
\cite{fridrichOnehot} identifica e resolve uma limitação específica das CNNs em
esteganálise. A pesquisa parte da descoberta de cenários específicos onde as
modernas CNNs, como a SRNet, apresentavam um desempenho surpreendentemente
inferior ao de métodos mais antigos baseados em extração de características,
como o JPEG Rich Model (JRM). Essa falha era particularmente evidente na
detecção do algoritmo nsF5 e do J-UNIWARD em certos tipos de imagem JPEG, e a
análise revelou que o sucesso do JRM nesses casos se devia à sua capacidade de
computar estatísticas simples dos coeficientes DCT, como histogramas de
coocorrência, algo que as CNNs convencionais, que operam na imagem
descomprimida, não conseguiam "enxergar".

A metodologia se baseia em duas inovações principais. A primeira é a própria
rede "OneHot CNN", que transforma sua entrada através de uma camada de
"codificação one-hot com corte" (clipped one-hot encoding). Nessa etapa, os
valores absolutos dos coeficientes DCT são transformados em um volume binário
que facilita o aprendizado de ocorrências e coocorrências pelas camadas
convolucionais subsequentes. A segunda inovação é a arquitetura de ramo duplo
"OneHot+SRNet", que mescla a nova rede OneHot com uma CNN convencional (SRNet).
O experimento principal consistiu em testar o desempenho dessas novas
arquiteturas nos cenários problemáticos (nsF5 e J-UNIWARD) em datasets como
BOSSbase e BOWS2.

O trabalho se destaca por identificar com precisão uma falha em modelos estado
da arte e propor uma solução elegante e eficaz, a codificação one-hot, que
permite a uma CNN aprender estatísticas de alta ordem de forma flexível. A
arquitetura de ramo duplo é outro ponto forte, pois oferece uma maneira prática
de criar um detector mais completo e robusto. Uma limitação implícita é que a
rede OneHot é altamente especializada para esses casos de falha, o que
justifica sua fusão com uma rede mais geral como a SRNet.

%trabalho 5
Para contextualizar os trabalhos anteriores dentro do panorama geral da área, o
artigo \textit{Comprehensive survey on image steganalysis using deep learning}
de De La Croix et al. \cite{LaCroix2024survey} oferece uma visão abrangente do
estado da arte. A principal motivação dos autores reside na observação de que
as abordagens tradicionais, baseadas em aprendizado de máquina (ML), se
mostraram ineficazes contra os modernos algoritmos de esteganografia. Métodos
clássicos como Support Vector Machines (SVM) e Ensemble Classifiers (EC)
dependem de um processo árduo e manual de extração de características, o que
não só consome tempo, mas também sofre com a "maldição da dimensionalidade",
onde o excesso de características prejudica o desempenho do classificador. A
introdução do aprendizado profundo (deep learning) marcou uma mudança de
paradigma, unificando a extração de características e a classificação em um
único processo otimizado e de ponta a ponta.

A metodologia empregada pelos autores é a de uma revisão sistemática da
literatura, baseada no protocolo PRISMA. Eles selecionaram 24 artigos de ponta,
publicados entre 2014 e 2023, que representam a vanguarda da esteganálise com
deep learning. O artigo estrutura-se de forma didática, iniciando com uma
taxonomia detalhada das técnicas de esteganálise, explorando a transição do
paradigma de ML para o de deep learning, e analisando as arquiteturas de Redes
Neurais Convolucionais (CNNs) propostas, desde as pioneiras até as mais
recentes. A pesquisa documenta a progressão das arquiteturas, começando com
modelos como Qian-Net, passando pelo Xu-Net, chegando a modelos mais
sofisticados como o Ye-Net, e culminando em arquiteturas estado da arte como a
SRNet e a GBRAS-Net.

Um dos pontos fortes mais significativos desta revisão é a identificação dos
principais desafios que ainda persistem no campo da esteganálise com deep
learning: vulnerabilidade a ataques adversariais, qualidade e padronização dos
datasets, ineficiência para lidar com imagens de tamanhos arbitrários,
dificuldade na detecção de baixo payload, problema do cover-source mismatch,
dificuldade na identificação de características globais, e necessidade de um
grande número de amostras de treinamento. Como limitação, a revisão foca
principalmente em imagens no domínio espacial, deixando lacunas para outros
domínios como JPEG.

%trabalho 6
A pesquisa de Farooq e Selwal \cite{farooq2023systematic} é motivada pela
crescente complexidade e sofisticação tanto das técnicas de esteganografia
quanto das de esteganálise. Os autores partem da premissa de que, com os
avanços em comunicação e tecnologia da informação, os métodos para ocultar
dados em imagens tornaram-se mais robustos, desafiando as abordagens de
detecção tradicionais \cite{farooq2023systematic}. Enquanto métodos clássicos
se baseiam em um processo de duas etapas—extração manual de características
(como os Rich Models) e classificação (usando SVMs ou Ensemble Classifiers)—, o
aprendizado profundo (deep learning, DL) emergiu como uma alternativa superior,
unificando essas etapas e aprendendo automaticamente as características mais
relevantes diretamente dos dados brutos \cite{farooq2023systematic}. Essa
mudança de paradigma gerou uma vasta quantidade de novas arquiteturas e
abordagens, criando a necessidade de uma análise consolidada. Assim, o
principal objetivo do trabalho é realizar uma revisão sistemática e aprofundada
das técnicas de esteganálise de imagens baseadas em DL, oferecendo um panorama
do estado da arte, analisando comparativamente os modelos, os datasets de
referência e as métricas de avaliação, além de identificar os desafios de
pesquisa que permanecem abertos para guiar futuros investigadores no campo
\cite{farooq2023systematic}.

A metodologia adotada no artigo é a de uma revisão sistemática e abrangente da
literatura. Primeiramente, os autores fornecem uma classificação detalhada das
técnicas de esteganálise, cobrindo desde ataques visuais e estatísticos até
abordagens mais complexas como a análise estrutural e métodos baseados em redes
neurais artificiais (ANN), estabelecendo um contexto histórico e técnico
\cite{farooq2023systematic}. O foco principal, no entanto, é a análise
cronológica das abordagens baseadas em DL, com especial atenção às Redes
Neurais Convolucionais (CNNs). O artigo revisa uma sequência de trabalhos
influentes, começando com as primeiras propostas de Tan et al. (2014) e Qian et
al. (2015), e avançando para arquiteturas mais complexas e eficazes como as de
Xu et al. (2016), Ye et al. (2017), Yedroudj et al. (2018), e a SRNet de
Boroumand et al. (2019) \cite{farooq2023systematic}. Para cada trabalho, os
autores analisam a arquitetura da rede, as funções de ativação utilizadas, as
bases de dados (como BOSSBase, BOWS2 e ImageNet), os algoritmos
esteganográficos alvo (S-UNIWARD, WOW, HILL, etc.) e os resultados de
desempenho reportados. Além disso, a pesquisa compila e discute os principais
datasets de referência, as métricas de avaliação quantitativas (como taxa de
erro, curva ROC, AUC e WAUC) e qualitativas (PSNR, SSIM), e até mesmo lista
ferramentas de software de código aberto e proprietárias disponíveis para
esteganálise \cite{farooq2023systematic}.

Como um artigo de revisão, o experimento consiste na compilação e na análise
crítica dos resultados publicados por outros pesquisadores. A pesquisa
consolida os dados de desempenho de diversas arquiteturas de CNN em uma tabela
comparativa detalhada, avaliando a taxa de erro de detecção contra diferentes
algoritmos esteganográficos (como HUGO, WOW, S-UNIWARD) e em diferentes taxas
de inserção de dados (payloads) \cite{farooq2023systematic}. A análise dos
resultados mostra uma clara evolução: os primeiros modelos de CNN apresentavam
desempenho comparável ou ligeiramente inferior aos métodos tradicionais (como o
SRM com Ensemble Classifier), mas com o avanço no design das
arquiteturas—incorporando camadas de pré-processamento com filtros, funções de
ativação mais adequadas (como a TLU), normalização em lote (BN), e arquiteturas
residuais profundas—, os modelos mais recentes superaram significativamente os
métodos clássicos, especialmente em cenários com maior payload
\cite{farooq2023systematic}. O artigo destaca, por exemplo, o desempenho
superior de modelos como o de Yedroudj et al. e a SRNet, que estabeleceram
novos padrões de acurácia na detecção \cite{farooq2023systematic}.

Os pontos fortes deste artigo são a sua abrangência e o seu caráter
sistemático. Ele funciona como um documento de referência completo, não apenas
listando os trabalhos, mas contextualizando-os, explicando a evolução das
ideias e das arquiteturas, e oferecendo uma análise comparativa clara. Outro
ponto forte crucial é a identificação detalhada das limitações e dos desafios
de pesquisa em aberto, que incluem: a dificuldade na detecção de
características estatísticas de sinais esteganográficos fracos; o desafio
persistente da detecção em cenários de baixo payload; os problemas de
generalização (como steganographic mismatch, cover-source mismatch e payload
mismatch); e a necessidade de mais pesquisas em esteganálise quantitativa e
locativa usando DL \cite{farooq2023systematic}. Uma limitação inerente a
qualquer trabalho de revisão é que ele depende da qualidade e da consistência
dos dados reportados pelos artigos originais, não gerando novos resultados
empíricos por si só.

%trabalho 8
Buscando avançar o estado da arte em esteganálise no domínio espacial, o artigo
\textit{A convolutional neural network to detect possible hidden data in
  spatial domain images} de La Croix \cite{croix2023CNN} propõe uma nova
arquitetura de rede neural convolucional. A principal motivação dos autores é a
observação de que, embora as CNNs existentes tenham superado as técnicas
clássicas, elas ainda apresentam problemas de desempenho em termos de acurácia
de classificação e estabilidade durante o treinamento. O objetivo do trabalho
é, portanto, projetar uma nova arquitetura que enderece essas lacunas,
melhorando tanto a precisão na detecção de dados ocultos quanto a estabilidade
do treinamento, através de inovações específicas nas fases de
pré-processamento, extração de características e classificação.

A metodologia consiste no desenvolvimento de uma CNN customizada que se inicia
com filtros não treináveis do Spatial Rich Model (SRM) para amplificar o ruído
esteganográfico. A principal inovação está na fase de extração de
características, onde se utiliza uma combinação de convoluções separáveis em
profundidade (depthwise separable convolutions) com a função de ativação Leaky
Rectified Linear Unit (LReLU) para reduzir parâmetros, evitar o sobreajuste e
melhorar a estabilidade do gradiente. Na classificação, um módulo de pooling
multi-escala agrega as características de forma eficiente. O experimento
principal comparou o desempenho da rede proposta com modelos de ponta (Ye-Net,
GBRAS-Net) nos datasets BOSSbase e BOWS 2 contra os algoritmos S-UNIWARD e WOW,
demonstrando consistentemente uma performance superior.

Um dos pontos fortes mais significativos do trabalho é o ganho de desempenho
expressivo sobre o estado da arte, com melhorias na acurácia que chegam a mais
de 10\% em alguns cenários. Os autores validam rigorosamente a contribuição de
cada componente inovador através de um estudo de ablação, que demonstra que o
uso de convoluções separáveis, a ativação LReLU e o pooling multi-escala,
individualmente, trazem ganhos substanciais para o resultado final. Como
limitação, a avaliação do artigo se concentra exclusivamente em algoritmos do
domínio espacial, não explorando o desempenho da arquitetura em domínios de
transformação como o JPEG, e embora apresente resultados preliminares para
imagens de tamanhos arbitrários, uma análise aprofundada desse cenário é
declarada como fora do escopo do trabalho.

% Isso daqui vai ser nossa "conclusão" da revisão bibliográfica, já q é basicamente isso q a gente tem de diferente
Em contraste com os trabalhos revisados, que focam em ....., a proposta
desenvolvida neste trabalho busca explorar o uso de bibliotecas de software
livre para a construção de modelos de inteligência artificial em esteganálise,
democratizando a área ao garantir reprodutibilidade, baixo custo e
acessibilidade.

\section{Metodologia}

\section{Resultados e discussões}

\section{Conclusão}\label{sec:figs}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=.5\textwidth]{fig1.jpg}
% \caption{A typical figure}
% \label{fig:exampleFig1}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=.3\textwidth]{fig2.jpg}
% \caption{This figure is an example of a figure caption taking more than one
%   line and justified considering margins mentioned in Section~\ref{sec:figs}.}
% \label{fig:exampleFig2}
% \end{figure}

% \begin{table}[ht]
% \centering
% \caption{Variables to be considered on the evaluation of interaction
%   techniques}
% \label{tab:exTable1}
% \includegraphics[width=.7\textwidth]{table.jpg}
% \end{table}

\bibliographystyle{include/template/sbc}
\bibliography{include/bibliography/bibliography}

\end{document}
